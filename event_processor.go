package ldclient

import (
	"bytes"
	"encoding/json"
	"io/ioutil"
	"math/rand"
	"net/http"
	"sync"
	"sync/atomic"
	"time"
)

type EventProcessor interface {
	// Records an event asynchronously.
	SendEvent(Event)
	// Specifies that any buffered events should be sent as soon as possible, rather than waiting
	// for the next flush interval. This method is asynchronous, so events still may not be sent
	// until a later time.
	Flush()
	// Shuts down all event processor activity, after first ensuring that all events have been
	// delivered. Subsequent calls to SendEvent() or Flush() will be ignored.
	Close()
}

type nullEventProcessor struct{}

type defaultEventProcessor struct {
	inputCh    chan eventDispatcherMessage
	dispatcher *eventDispatcher
	closeOnce  sync.Once
}

type eventDispatcher struct {
	sdkKey            string
	config            Config
	client            *http.Client
	inputCh           chan eventDispatcherMessage
	events            []interface{}
	summarizer        *eventSummarizer
	userKeys          lruCache
	lastKnownPastTime uint64
	capacityExceeded  bool
	flushCh           chan *flushPayload
	workersGroup      sync.WaitGroup
	disabled          int32 // Really a bool, but there's no atomic bool
}

type flushPayload struct {
	events  []interface{}
	summary eventSummary
}

// Payload of the inputCh channel.
type eventDispatcherMessage interface{}

type sendEventMessage struct {
	event Event
}

type flushEventsMessage struct{}

type shutdownEventsMessage struct {
	replyCh chan struct{}
}

type syncEventsMessage struct {
	replyCh chan struct{}
}

// Serializable form of a feature request event. This differs from the event that was
// passed in to us in that it has a user key instead of a user object, and it only shows
// the flag value, not the variation index.
type featureRequestEventOutput struct {
	Kind         string      `json:"kind"`
	CreationDate uint64      `json:"creationDate"`
	Key          string      `json:"key"`
	UserKey      *string     `json:"userKey,omitempty"`
	User         *User       `json:"user,omitempty"`
	Value        interface{} `json:"value"`
	Default      interface{} `json:"default"`
	Version      *int        `json:"version,omitempty"`
	PrereqOf     *string     `json:"prereqOf,omitempty"`
}

// Serializable form of an identify event.
type identifyEventOutput struct {
	Kind         string  `json:"kind"`
	CreationDate uint64  `json:"creationDate"`
	Key          *string `json:"key"`
	User         *User   `json:"user"`
}

// Serializable form of a custom event. It has a user key instead of a user object.
type customEventOutput struct {
	Kind         string      `json:"kind"`
	CreationDate uint64      `json:"creationDate"`
	Key          string      `json:"key"`
	UserKey      *string     `json:"userKey,omitempty"`
	User         *User       `json:"user,omitempty"`
	Data         interface{} `json:"data,omitempty"`
}

// Serializable form of an index event. This is not generated by an explicit client call,
// but is created automatically whenever we see a user we haven't seen before in a feature
// request event or custom event.
type indexEventOutput struct {
	Kind         string `json:"kind"`
	CreationDate uint64 `json:"creationDate"`
	User         *User  `json:"user"`
}

// Serializable form of a summary event, containing data generated by EventSummarizer.
type summaryEventOutput struct {
	Kind      string                     `json:"kind"`
	StartDate uint64                     `json:"startDate"`
	EndDate   uint64                     `json:"endDate"`
	Features  map[string]flagSummaryData `json:"features"`
}

const (
	FEATURE_REQUEST_EVENT = "feature"
	FEATURE_DEBUG_EVENT   = "debug"
	CUSTOM_EVENT          = "custom"
	IDENTIFY_EVENT        = "identify"
	INDEX_EVENT           = "index"
	SUMMARY_EVENT         = "summary"
	maxFlushWorkers       = 5
)

func newNullEventProcessor() *nullEventProcessor {
	return &nullEventProcessor{}
}

func (n *nullEventProcessor) SendEvent(e Event) {}

func (n *nullEventProcessor) Flush() {}

func (n *nullEventProcessor) Close() {}

func newDefaultEventProcessor(sdkKey string, config Config, client *http.Client) *defaultEventProcessor {
	if client == nil {
		client = &http.Client{}
	}
	inputCh := make(chan eventDispatcherMessage, config.Capacity)
	dispatcher := &eventDispatcher{
		sdkKey:     sdkKey,
		config:     config,
		inputCh:    inputCh,
		events:     make([]interface{}, 0),
		client:     client,
		flushCh:    make(chan *flushPayload, 1),
		summarizer: newEventSummarizer(),
		userKeys:   newLruCache(config.UserKeysCapacity),
	}
	res := &defaultEventProcessor{
		inputCh:    inputCh,
		dispatcher: dispatcher,
	}
	dispatcher.start()
	return res
}

func (ep *defaultEventProcessor) SendEvent(e Event) {
	ep.inputCh <- sendEventMessage{event: e}
}

func (ep *defaultEventProcessor) Flush() {
	ep.inputCh <- flushEventsMessage{}
}

func (ep *defaultEventProcessor) Close() {
	ep.closeOnce.Do(func() {
		ep.inputCh <- flushEventsMessage{}
		m := shutdownEventsMessage{replyCh: make(chan struct{})}
		ep.inputCh <- m
		<-m.replyCh
	})
}

// used only for testing - ensures that all pending messages and flushes have completed
func (ep *defaultEventProcessor) waitUntilInactive() {
	m := syncEventsMessage{replyCh: make(chan struct{})}
	ep.inputCh <- m
	<-m.replyCh // Now we know that all events prior to this call have been processed
	ep.dispatcher.workersGroup.Wait()
}

func (ec *eventDispatcher) start() {
	// Start a fixed-size pool of workers that wait on flushTriggerCh. This is the
	// maximum number of flushes we can do concurrently.
	for i := 0; i < maxFlushWorkers; i++ {
		go ec.runFlushWorker(i)
	}
	go ec.runMainLoop()
}

func (ec *eventDispatcher) runMainLoop() {
	if err := recover(); err != nil {
		ec.config.Logger.Printf("Unexpected panic in event processing thread: %+v", err)
	}

	flushInterval := ec.config.FlushInterval
	if flushInterval <= 0 {
		flushInterval = DefaultConfig.FlushInterval
	}
	userKeysFlushInterval := ec.config.UserKeysFlushInterval
	if userKeysFlushInterval <= 0 {
		userKeysFlushInterval = DefaultConfig.UserKeysFlushInterval
	}
	flushTicker := time.NewTicker(flushInterval)
	usersResetTicker := time.NewTicker(userKeysFlushInterval)

	for {
		select {
		case message := <-ec.inputCh:
			switch m := message.(type) {
			case sendEventMessage:
				ec.processEvent(m.event)
			case flushEventsMessage:
				ec.triggerFlush()
			case syncEventsMessage:
				m.replyCh <- struct{}{}
			case shutdownEventsMessage:
				flushTicker.Stop()
				usersResetTicker.Stop()
				ec.workersGroup.Wait() // Wait for all in-progress flushes to complete
				close(ec.flushCh)      // Causes all idle flush workers to terminate
				m.replyCh <- struct{}{}
				return
			}
		case <-flushTicker.C:
			ec.triggerFlush()
		case <-usersResetTicker.C:
			ec.userKeys.clear()
		}
	}
}

func (ec *eventDispatcher) runFlushWorker(n int) {
	for {
		payload, more := <-ec.flushCh
		if !more {
			// Channel has been closed - we're shutting down
			break
		}
		ec.doFlush(payload)
		ec.workersGroup.Done() // Decrement the count of in-progress flushes
	}
}

func (ec *eventDispatcher) isDisabled() bool {
	return atomic.LoadInt32(&ec.disabled) != 0
}

func (ec *eventDispatcher) processEvent(evt Event) {
	if ec.isDisabled() {
		return
	}
	// For each user we haven't seen before, we add an index event - unless this is already
	// an identify event for that user.
	if !ec.config.InlineUsersInEvents {
		user := evt.GetBase().User
		if !ec.noticeUser(&user) {
			if _, ok := evt.(IdentifyEvent); !ok {
				indexEvent := indexEventOutput{
					Kind:         INDEX_EVENT,
					CreationDate: evt.GetBase().CreationDate,
					User:         &user,
				}
				ec.queueEvent(indexEvent)
			}
		}
	}

	// Always record the event in the summarizer.
	ec.summarizer.summarizeEvent(evt)

	if ec.shouldTrackFullEvent(evt) {
		// Sampling interval applies only to fully-tracked events.
		if ec.config.SamplingInterval == 0 || rand.Int31n(ec.config.SamplingInterval) == 0 {
			// Queue the event as-is; we'll transform it into an output event when we're flushing
			// (to avoid doing that work on our main goroutine).
			ec.queueEvent(evt)
		}
	}
}

// Add to the set of users we've noticed, and return true if the user was already known to us.
func (ec *eventDispatcher) noticeUser(user *User) bool {
	if user == nil || user.Key == nil {
		return true
	}
	return ec.userKeys.add(*user.Key)
}

func (ec *eventDispatcher) shouldTrackFullEvent(evt Event) bool {
	switch evt := evt.(type) {
	case FeatureRequestEvent:
		if evt.TrackEvents {
			return true
		}
		if evt.DebugEventsUntilDate != nil {
			// The "last known past time" comes from the last HTTP response we got from the server.
			// In case the client's time is set wrong, at least we know that any expiration date
			// earlier than that point is definitely in the past.  If there's any discrepancy, we
			// want to err on the side of cutting off event debugging sooner.
			lastPast := atomic.LoadUint64(&ec.lastKnownPastTime)
			if *evt.DebugEventsUntilDate > lastPast &&
				*evt.DebugEventsUntilDate > now() {
				return true
			}
		}
		return false
	default:
		// Custom and identify events are always included in full
		return true
	}
}

func (ec *eventDispatcher) queueEvent(event interface{}) {
	if len(ec.events) >= ec.config.Capacity {
		if !ec.capacityExceeded {
			ec.capacityExceeded = true
			ec.config.Logger.Printf("WARN: Exceeded event queue capacity. Increase capacity to avoid dropping events.")
		}
	} else {
		ec.capacityExceeded = false
		ec.events = append(ec.events, event)
	}
}

// Signal that we would like to do a flush as soon as possible.
func (ec *eventDispatcher) triggerFlush() {
	if ec.isDisabled() {
		return
	}
	// Is there anything to flush?
	summary := ec.summarizer.snapshot()
	if len(ec.events) == 0 && len(summary.counters) == 0 {
		return
	}
	payload := flushPayload{
		events:  ec.events,
		summary: summary,
	}
	ec.workersGroup.Add(1) // Increment the count of active flushes
	select {
	case ec.flushCh <- &payload:
		// If the channel wasn't full, then there is a worker available who will pick up
		// this flush payload and send it. The event buffer and summary state can now be
		// cleared from the main goroutine.
		ec.events = make([]interface{}, 0)
		ec.summarizer.reset()
	default:
		// We can't start a flush right now because we're waiting for one of the workers
		// to pick up the last one.  Do not reset the event buffer or summary state.
		ec.workersGroup.Done()
	}
}

// Runs on a flush worker thread
func (ec *eventDispatcher) doFlush(payload *flushPayload) {
	if len(payload.events) == 0 && len(payload.summary.counters) == 0 {
		return
	}
	outputEvents := make([]interface{}, 0, len(payload.events)+1) // leave room for summary, if any
	userFilter := newUserFilter(ec.config)
	for _, e := range payload.events {
		oe := ec.makeOutputEvent(e, &userFilter)
		if oe != nil {
			outputEvents = append(outputEvents, oe)
		}
	}
	if len(payload.summary.counters) > 0 {
		outputEvents = append(outputEvents, ec.makeSummaryEvent(payload.summary))
	}

	jsonPayload, marshalErr := json.Marshal(outputEvents)
	if marshalErr != nil {
		ec.config.Logger.Printf("Unexpected error marshalling event json: %+v", marshalErr)
		return
	}

	eventsUri := ec.config.EventsUri + "/bulk"
	req, reqErr := http.NewRequest("POST", eventsUri, bytes.NewReader(jsonPayload))
	if reqErr != nil {
		ec.config.Logger.Printf("Unexpected error while creating event request: %+v", reqErr)
		return
	}

	req.Header.Add("Authorization", ec.sdkKey)
	req.Header.Add("Content-Type", "application/json")
	req.Header.Add("User-Agent", ec.config.UserAgent)

	resp, respErr := ec.client.Do(req)

	defer func() {
		if resp != nil && resp.Body != nil {
			ioutil.ReadAll(resp.Body)
			resp.Body.Close()
		}
	}()

	if respErr != nil {
		ec.config.Logger.Printf("Unexpected error while sending events: %+v", respErr)
		return
	}
	ec.handleResponse(resp)
}

func (ec *eventDispatcher) handleResponse(resp *http.Response) {
	err := checkStatusCode(resp.StatusCode, resp.Request.URL.String())
	if err != nil {
		ec.config.Logger.Printf("Unexpected status code when sending events: %+v", err)
		if err != nil && err.Code == 401 {
			ec.config.Logger.Printf("Received 401 error, no further events will be posted since SDK key is invalid")
			atomic.StoreInt32(&ec.disabled, 1)
		}
	} else {
		dt, err := http.ParseTime(resp.Header.Get("Date"))
		if err == nil {
			tm := toUnixMillis(dt)
			atomic.StoreUint64(&ec.lastKnownPastTime, tm)
		}
	}
}

func (ec *eventDispatcher) makeOutputEvent(evt interface{}, uf *userFilter) interface{} {
	switch evt := evt.(type) {
	case FeatureRequestEvent:
		fe := featureRequestEventOutput{
			CreationDate: evt.BaseEvent.CreationDate,
			Key:          evt.Key,
			Value:        evt.Value,
			Default:      evt.Default,
			Version:      evt.Version,
			PrereqOf:     evt.PrereqOf,
		}
		if ec.config.InlineUsersInEvents {
			fe.User = uf.scrubUser(evt.User)
		} else {
			fe.UserKey = evt.User.Key
		}
		if !evt.TrackEvents && evt.DebugEventsUntilDate != nil {
			fe.Kind = FEATURE_DEBUG_EVENT
		} else {
			fe.Kind = FEATURE_REQUEST_EVENT
		}
		return fe
	case CustomEvent:
		ce := customEventOutput{
			Kind:         CUSTOM_EVENT,
			CreationDate: evt.BaseEvent.CreationDate,
			Key:          evt.Key,
			Data:         evt.Data,
		}
		if ec.config.InlineUsersInEvents {
			ce.User = uf.scrubUser(evt.User)
		} else {
			ce.UserKey = evt.User.Key
		}
		return ce
	case IdentifyEvent:
		return identifyEventOutput{
			Kind:         IDENTIFY_EVENT,
			CreationDate: evt.BaseEvent.CreationDate,
			Key:          evt.User.Key,
			User:         uf.scrubUser(evt.User),
		}
	case indexEventOutput:
		evt.User = uf.scrubUser(*evt.User)
		return evt
	default:
		ec.config.Logger.Printf("Found unknown event type in output queue: %T", evt)
		return nil
	}
}

// Transforms the summary data into the format used for event sending.
func (ec *eventDispatcher) makeSummaryEvent(snapshot eventSummary) summaryEventOutput {
	features := make(map[string]flagSummaryData)
	for key, value := range snapshot.counters {
		var flagData flagSummaryData
		var known bool
		if flagData, known = features[key.key]; !known {
			flagData = flagSummaryData{
				Default:  value.flagDefault,
				Counters: make([]flagCounterData, 0, 2),
			}
		}
		data := flagCounterData{
			Value: value.flagValue,
			Count: value.count,
		}
		if key.version == 0 {
			unknown := true
			data.Unknown = &unknown
		} else {
			version := key.version
			data.Version = &version
		}
		flagData.Counters = append(flagData.Counters, data)
		features[key.key] = flagData
	}

	return summaryEventOutput{
		Kind:      "summary",
		StartDate: snapshot.startDate,
		EndDate:   snapshot.endDate,
		Features:  features,
	}
}
